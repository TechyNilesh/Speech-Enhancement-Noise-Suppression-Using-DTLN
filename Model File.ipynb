{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc3f6e5",
   "metadata": {},
   "source": [
    "## Importing the Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bfadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, fnmatch\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Activation, Dense, LSTM, Dropout, \\\n",
    "    Lambda, Input, Multiply, Layer, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, CSVLogger, \\\n",
    "    EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "from wavinfo import WavInfoReader\n",
    "from random import shuffle, seed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2ec46",
   "metadata": {},
   "source": [
    "## Importing Defining Audio Generator Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed270ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class audio_generator():\n",
    "    '''\n",
    "    Class to create a Tensorflow dataset based on an iterator from a large scale \n",
    "    audio dataset. This audio generator only supports single channel audio files.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, path_to_input, path_to_s1, len_of_samples, fs, train_flag=False):\n",
    "        '''\n",
    "        Constructor of the audio generator class.\n",
    "        Inputs:\n",
    "            path_to_input       path to the mixtures\n",
    "            path_to_s1          path to the target source data\n",
    "            len_of_samples      length of audio snippets in samples\n",
    "            fs                  sampling rate\n",
    "            train_flag          flag for activate shuffling of files\n",
    "        '''\n",
    "        # set inputs to properties\n",
    "        self.path_to_input = path_to_input\n",
    "        self.path_to_s1 = path_to_s1\n",
    "        self.len_of_samples = len_of_samples\n",
    "        self.fs = fs\n",
    "        self.train_flag=train_flag\n",
    "        # count the number of samples in your data set (depending on your disk,\n",
    "        #                                               this can take some time)\n",
    "        self.count_samples()\n",
    "        # create iterable tf.data.Dataset object\n",
    "        self.create_tf_data_obj()\n",
    "        \n",
    "    def count_samples(self):\n",
    "        '''\n",
    "        Method to list the data of the dataset and count the number of samples. \n",
    "        '''\n",
    "\n",
    "        # list .wav files in directory\n",
    "        self.file_names = fnmatch.filter(os.listdir(self.path_to_input), '*.wav')\n",
    "        # count the number of samples contained in the dataset\n",
    "        self.total_samples = 0\n",
    "        for file in self.file_names:\n",
    "            info = WavInfoReader(os.path.join(self.path_to_input, file))\n",
    "            self.total_samples = self.total_samples + \\\n",
    "                int(np.fix(info.data.frame_count/self.len_of_samples))\n",
    "    \n",
    "         \n",
    "    def create_generator(self):\n",
    "        '''\n",
    "        Method to create the iterator. \n",
    "        '''\n",
    "\n",
    "        # check if training or validation\n",
    "        if self.train_flag:\n",
    "            shuffle(self.file_names)\n",
    "        # iterate over the files  \n",
    "        for file in self.file_names:\n",
    "            # read the audio files\n",
    "            noisy, fs_1 = sf.read(os.path.join(self.path_to_input, file))\n",
    "            speech, fs_2 = sf.read(os.path.join(self.path_to_s1, file))\n",
    "            # check if the sampling rates are matching the specifications\n",
    "            if fs_1 != self.fs or fs_2 != self.fs:\n",
    "                raise ValueError('Sampling rates do not match.')\n",
    "            if noisy.ndim != 1 or speech.ndim != 1:\n",
    "                raise ValueError('Too many audio channels. The DTLN audio_generator \\\n",
    "                                 only supports single channel audio data.')\n",
    "            # count the number of samples in one file\n",
    "            num_samples = int(np.fix(noisy.shape[0]/self.len_of_samples))\n",
    "            # iterate over the number of samples\n",
    "            for idx in range(num_samples):\n",
    "                # cut the audio files in chunks\n",
    "                in_dat = noisy[int(idx*self.len_of_samples):int((idx+1)*\n",
    "                                                        self.len_of_samples)]\n",
    "                tar_dat = speech[int(idx*self.len_of_samples):int((idx+1)*\n",
    "                                                        self.len_of_samples)]\n",
    "                # yield the chunks as float32 data\n",
    "                yield in_dat.astype('float32'), tar_dat.astype('float32')\n",
    "              \n",
    "\n",
    "    def create_tf_data_obj(self):\n",
    "        '''\n",
    "        Method to to create the tf.data.Dataset. \n",
    "        '''\n",
    "\n",
    "        # creating the tf.data.Dataset from the iterator\n",
    "        self.tf_data_set = tf.data.Dataset.from_generator(\n",
    "                        self.create_generator,\n",
    "                        (tf.float32, tf.float32),\n",
    "                        output_shapes=(tf.TensorShape([self.len_of_samples]), \\\n",
    "                                       tf.TensorShape([self.len_of_samples])),\n",
    "                        args=None\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bebe1",
   "metadata": {},
   "source": [
    "## Importing Defining DTLN Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d486c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTLN_model():\n",
    "    '''\n",
    "    Class to create and train the DTLN model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "\n",
    "        # defining default cost function\n",
    "        self.cost_function = self.snr_cost\n",
    "        # empty property for the model\n",
    "        self.model = []\n",
    "        # defining default parameters\n",
    "        self.fs = 16000\n",
    "        self.batchsize = 32\n",
    "        self.len_samples = 15\n",
    "        self.activation = 'sigmoid'\n",
    "        self.numUnits = 128\n",
    "        self.numLayer = 2\n",
    "        self.blockLen = 512\n",
    "        self.block_shift = 128\n",
    "        self.dropout = 0.25\n",
    "        self.lr = 1e-3\n",
    "        self.max_epochs = 200\n",
    "        self.encoder_size = 256\n",
    "        self.eps = 1e-7\n",
    "        # reset all seeds to 42 to reduce invariance between training runs\n",
    "        os.environ['PYTHONHASHSEED']=str(42)\n",
    "        seed(42)\n",
    "        np.random.seed(42)\n",
    "        tf.random.set_seed(42)\n",
    "        # some line to correctly find some libraries in TF 2.x\n",
    "        physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if len(physical_devices) > 0:\n",
    "            for device in physical_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, enable=True)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def snr_cost(s_estimate, s_true):\n",
    "        '''\n",
    "        Static Method defining the cost function. \n",
    "        The negative signal to noise ratio is calculated here. The loss is \n",
    "        always calculated over the last dimension. \n",
    "        '''\n",
    "\n",
    "        # calculating the SNR\n",
    "        snr = tf.reduce_mean(tf.math.square(s_true), axis=-1, keepdims=True) / \\\n",
    "            (tf.reduce_mean(tf.math.square(s_true-s_estimate), axis=-1, keepdims=True)+1e-7)\n",
    "        # using some more lines, because TF has no log10\n",
    "        num = tf.math.log(snr) \n",
    "        denom = tf.math.log(tf.constant(10, dtype=num.dtype))\n",
    "        loss = -10*(num / (denom))\n",
    "        # returning the loss\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def lossWrapper(self):\n",
    "        '''\n",
    "        A wrapper function which returns the loss function. This is done to\n",
    "        to enable additional arguments to the loss function if necessary.\n",
    "        '''\n",
    "        def lossFunction(y_true,y_pred):\n",
    "            # calculating loss and squeezing single dimensions away\n",
    "            loss = tf.squeeze(self.cost_function(y_pred,y_true))\n",
    "            # calculate mean over batches\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            # return the loss\n",
    "            return loss\n",
    "        # returning the loss function as handle\n",
    "        return lossFunction\n",
    "    \n",
    "    \n",
    "\n",
    "    '''\n",
    "    In the following some helper layers are defined.\n",
    "    '''  \n",
    "    \n",
    "    def stftLayer(self, x):\n",
    "        '''\n",
    "        Method for an STFT helper layer used with a Lambda layer. The layer\n",
    "        calculates the STFT on the last dimension and returns the magnitude and\n",
    "        phase of the STFT.\n",
    "        '''\n",
    "        \n",
    "        # creating frames from the continuous waveform\n",
    "        frames = tf.signal.frame(x, self.blockLen, self.block_shift)\n",
    "        # calculating the fft over the time frames. rfft returns NFFT/2+1 bins.\n",
    "        stft_dat = tf.signal.rfft(frames)\n",
    "        # calculating magnitude and phase from the complex signal\n",
    "        mag = tf.abs(stft_dat)\n",
    "        phase = tf.math.angle(stft_dat)\n",
    "        # returning magnitude and phase as list\n",
    "        return [mag, phase]\n",
    "    \n",
    "    def fftLayer(self, x):\n",
    "        '''\n",
    "        Method for an fft helper layer used with a Lambda layer. The layer\n",
    "        calculates the rFFT on the last dimension and returns the magnitude and\n",
    "        phase of the STFT.\n",
    "        '''\n",
    "        \n",
    "        # expanding dimensions\n",
    "        frame = tf.expand_dims(x, axis=1)\n",
    "        # calculating the fft over the time frames. rfft returns NFFT/2+1 bins.\n",
    "        stft_dat = tf.signal.rfft(frame)\n",
    "        # calculating magnitude and phase from the complex signal\n",
    "        mag = tf.abs(stft_dat)\n",
    "        phase = tf.math.angle(stft_dat)\n",
    "        # returning magnitude and phase as list\n",
    "        return [mag, phase]\n",
    "\n",
    " \n",
    "        \n",
    "    def ifftLayer(self, x):\n",
    "        '''\n",
    "        Method for an inverse FFT layer used with an Lambda layer. This layer\n",
    "        calculates time domain frames from magnitude and phase information. \n",
    "        As input x a list with [mag,phase] is required.\n",
    "        '''\n",
    "        \n",
    "        # calculating the complex representation\n",
    "        s1_stft = (tf.cast(x[0], tf.complex64) * \n",
    "                    tf.exp( (1j * tf.cast(x[1], tf.complex64))))\n",
    "        # returning the time domain frames\n",
    "        return tf.signal.irfft(s1_stft)  \n",
    "    \n",
    "    \n",
    "    def overlapAddLayer(self, x):\n",
    "        '''\n",
    "        Method for an overlap and add helper layer used with a Lambda layer.\n",
    "        This layer reconstructs the waveform from a framed signal.\n",
    "        '''\n",
    "\n",
    "        # calculating and returning the reconstructed waveform\n",
    "        return tf.signal.overlap_and_add(x, self.block_shift)\n",
    "    \n",
    "        \n",
    "\n",
    "    def seperation_kernel(self, num_layer, mask_size, x, stateful=False):\n",
    "        '''\n",
    "        Method to create a separation kernel. \n",
    "        !! Important !!: Do not use this layer with a Lambda layer. If used with\n",
    "        a Lambda layer the gradients are updated correctly.\n",
    "\n",
    "        Inputs:\n",
    "            num_layer       Number of LSTM layers\n",
    "            mask_size       Output size of the mask and size of the Dense layer\n",
    "        '''\n",
    "\n",
    "        # creating num_layer number of LSTM layers\n",
    "        for idx in range(num_layer):\n",
    "            x = LSTM(self.numUnits, return_sequences=True, stateful=stateful)(x)\n",
    "            # using dropout between the LSTM layer for regularization \n",
    "            if idx<(num_layer-1):\n",
    "                x = Dropout(self.dropout)(x)\n",
    "        # creating the mask with a Dense and an Activation layer\n",
    "        mask = Dense(mask_size)(x)\n",
    "        mask = Activation(self.activation)(mask)\n",
    "        # returning the mask\n",
    "        return mask\n",
    "    \n",
    "    def seperation_kernel_with_states(self, num_layer, mask_size, x, \n",
    "                                      in_states):\n",
    "        '''\n",
    "        Method to create a separation kernel, which returns the LSTM states. \n",
    "        !! Important !!: Do not use this layer with a Lambda layer. If used with\n",
    "        a Lambda layer the gradients are updated correctly.\n",
    "\n",
    "        Inputs:\n",
    "            num_layer       Number of LSTM layers\n",
    "            mask_size       Output size of the mask and size of the Dense layer\n",
    "        '''\n",
    "        \n",
    "        states_h = []\n",
    "        states_c = []\n",
    "        # creating num_layer number of LSTM layers\n",
    "        for idx in range(num_layer):\n",
    "            in_state = [in_states[:,idx,:, 0], in_states[:,idx,:, 1]]\n",
    "            x, h_state, c_state = LSTM(self.numUnits, return_sequences=True, \n",
    "                     unroll=True, return_state=True)(x, initial_state=in_state)\n",
    "            # using dropout between the LSTM layer for regularization \n",
    "            if idx<(num_layer-1):\n",
    "                x = Dropout(self.dropout)(x)\n",
    "            states_h.append(h_state)\n",
    "            states_c.append(c_state)\n",
    "        # creating the mask with a Dense and an Activation layer\n",
    "        mask = Dense(mask_size)(x)\n",
    "        mask = Activation(self.activation)(mask)\n",
    "        out_states_h = tf.reshape(tf.stack(states_h, axis=0), \n",
    "                                  [1,num_layer,self.numUnits])\n",
    "        out_states_c = tf.reshape(tf.stack(states_c, axis=0), \n",
    "                                  [1,num_layer,self.numUnits])\n",
    "        out_states = tf.stack([out_states_h, out_states_c], axis=-1)\n",
    "        # returning the mask and states\n",
    "        return mask, out_states\n",
    "\n",
    "    def build_DTLN_model(self, norm_stft=False):\n",
    "        '''\n",
    "        Method to build and compile the DTLN model. The model takes time domain \n",
    "        batches of size (batchsize, len_in_samples) and returns enhanced clips \n",
    "        in the same dimensions. As optimizer for the Training process the Adam\n",
    "        optimizer with a gradient norm clipping of 3 is used. \n",
    "        The model contains two separation cores. The first has an STFT signal \n",
    "        transformation and the second a learned transformation based on 1D-Conv \n",
    "        layer. \n",
    "        '''\n",
    "        \n",
    "        # input layer for time signal\n",
    "        time_dat = Input(batch_shape=(None, None))\n",
    "        # calculate STFT\n",
    "        mag,angle = Lambda(self.stftLayer)(time_dat)\n",
    "        # normalizing log magnitude stfts to get more robust against level variations\n",
    "        if norm_stft:\n",
    "            mag_norm = InstantLayerNormalization()(tf.math.log(mag + 1e-7))\n",
    "        else:\n",
    "            # behaviour like in the paper\n",
    "            mag_norm = mag\n",
    "        # predicting mask with separation kernel  \n",
    "        mask_1 = self.seperation_kernel(self.numLayer, (self.blockLen//2+1), mag_norm)\n",
    "        # multiply mask with magnitude\n",
    "        estimated_mag = Multiply()([mag, mask_1])\n",
    "        # transform frames back to time domain\n",
    "        estimated_frames_1 = Lambda(self.ifftLayer)([estimated_mag,angle])\n",
    "        # encode time domain frames to feature domain\n",
    "        encoded_frames = Conv1D(self.encoder_size,1,strides=1,use_bias=False)(estimated_frames_1)\n",
    "        # normalize the input to the separation kernel\n",
    "        encoded_frames_norm = InstantLayerNormalization()(encoded_frames)\n",
    "        # predict mask based on the normalized feature frames\n",
    "        mask_2 = self.seperation_kernel(self.numLayer, self.encoder_size, encoded_frames_norm)\n",
    "        # multiply encoded frames with the mask\n",
    "        estimated = Multiply()([encoded_frames, mask_2]) \n",
    "        # decode the frames back to time domain\n",
    "        decoded_frames = Conv1D(self.blockLen, 1, padding='causal',use_bias=False)(estimated)\n",
    "        # create waveform with overlap and add procedure\n",
    "        estimated_sig = Lambda(self.overlapAddLayer)(decoded_frames)\n",
    "\n",
    "        \n",
    "        # create the model\n",
    "        self.model = Model(inputs=time_dat, outputs=estimated_sig)\n",
    "        # show the model summary\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def build_DTLN_model_stateful(self, norm_stft=False):\n",
    "        '''\n",
    "        Method to build stateful DTLN model for real time processing. The model \n",
    "        takes one time domain frame of size (1, blockLen) and one enhanced frame. \n",
    "         \n",
    "        '''\n",
    "        \n",
    "        # input layer for time signal\n",
    "        time_dat = Input(batch_shape=(1, self.blockLen))\n",
    "        # calculate STFT\n",
    "        mag,angle = Lambda(self.fftLayer)(time_dat)\n",
    "        # normalizing log magnitude stfts to get more robust against level variations\n",
    "        if norm_stft:\n",
    "            mag_norm = InstantLayerNormalization()(tf.math.log(mag + 1e-7))\n",
    "        else:\n",
    "            # behaviour like in the paper\n",
    "            mag_norm = mag\n",
    "        # predicting mask with separation kernel  \n",
    "        mask_1 = self.seperation_kernel(self.numLayer, (self.blockLen//2+1), mag_norm, stateful=True)\n",
    "        # multiply mask with magnitude\n",
    "        estimated_mag = Multiply()([mag, mask_1])\n",
    "        # transform frames back to time domain\n",
    "        estimated_frames_1 = Lambda(self.ifftLayer)([estimated_mag,angle])\n",
    "        # encode time domain frames to feature domain\n",
    "        encoded_frames = Conv1D(self.encoder_size,1,strides=1,use_bias=False)(estimated_frames_1)\n",
    "        # normalize the input to the separation kernel\n",
    "        encoded_frames_norm = InstantLayerNormalization()(encoded_frames)\n",
    "        # predict mask based on the normalized feature frames\n",
    "        mask_2 = self.seperation_kernel(self.numLayer, self.encoder_size, encoded_frames_norm, stateful=True)\n",
    "        # multiply encoded frames with the mask\n",
    "        estimated = Multiply()([encoded_frames, mask_2]) \n",
    "        # decode the frames back to time domain\n",
    "        decoded_frame = Conv1D(self.blockLen, 1, padding='causal',use_bias=False)(estimated)\n",
    "        # create the model\n",
    "        self.model = Model(inputs=time_dat, outputs=decoded_frame)\n",
    "        # show the model summary\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def compile_model(self):\n",
    "        '''\n",
    "        Method to compile the model for training\n",
    "\n",
    "        '''\n",
    "        \n",
    "        # use the Adam optimizer with a clipnorm of 3\n",
    "        optimizerAdam = keras.optimizers.Adam(lr=self.lr, clipnorm=3.0)\n",
    "        # compile model with loss function\n",
    "        self.model.compile(loss=self.lossWrapper(), optimizer=optimizerAdam)\n",
    "        \n",
    "    def create_saved_model(self, weights_file, target_name):\n",
    "        '''\n",
    "        Method to create a saved model folder from a weights file\n",
    "\n",
    "        '''\n",
    "        # check for type\n",
    "        if weights_file.find('_norm_') != -1:\n",
    "            norm_stft = True\n",
    "        else:\n",
    "            norm_stft = False\n",
    "        # build model    \n",
    "        self.build_DTLN_model_stateful(norm_stft=norm_stft)\n",
    "        # load weights\n",
    "        self.model.load_weights(weights_file)\n",
    "        # save model\n",
    "        tf.saved_model.save(self.model, target_name)\n",
    "    \n",
    "    def train_model(self, runName, path_to_train_mix, path_to_train_speech, \\\n",
    "                    path_to_val_mix, path_to_val_speech):\n",
    "        '''\n",
    "        Method to train the DTLN model. \n",
    "        '''\n",
    "        \n",
    "        # create save path if not existent\n",
    "        savePath = './models_'+ runName+'/' \n",
    "        if not os.path.exists(savePath):\n",
    "            os.makedirs(savePath)\n",
    "        # create log file writer\n",
    "        csv_logger = CSVLogger(savePath+ 'training_' +runName+ '.log')\n",
    "        # create callback for the adaptive learning rate\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=3, min_lr=10**(-10), cooldown=1)\n",
    "        # create callback for early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "            patience=10, verbose=0, mode='auto', baseline=None)\n",
    "        # create model check pointer to save the best model\n",
    "        checkpointer = ModelCheckpoint(savePath+runName+'.h5',\n",
    "                                       monitor='val_loss',\n",
    "                                       verbose=1,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=True,\n",
    "                                       mode='auto',\n",
    "                                       save_freq='epoch'\n",
    "                                       )\n",
    "\n",
    "        # calculate length of audio chunks in samples\n",
    "        len_in_samples = int(np.fix(self.fs * self.len_samples / \n",
    "                                    self.block_shift)*self.block_shift)\n",
    "        # create data generator for training data\n",
    "        generator_input = audio_generator(path_to_train_mix, \n",
    "                                          path_to_train_speech, \n",
    "                                          len_in_samples, \n",
    "                                          self.fs, train_flag=True)\n",
    "        dataset = generator_input.tf_data_set\n",
    "        dataset = dataset.batch(self.batchsize, drop_remainder=True).repeat()\n",
    "        # calculate number of training steps in one epoch\n",
    "        steps_train = generator_input.total_samples//self.batchsize\n",
    "        # create data generator for validation data\n",
    "        generator_val = audio_generator(path_to_val_mix,\n",
    "                                        path_to_val_speech, \n",
    "                                        len_in_samples, self.fs)\n",
    "        dataset_val = generator_val.tf_data_set\n",
    "        dataset_val = dataset_val.batch(self.batchsize, drop_remainder=True).repeat()\n",
    "        # calculate number of validation steps\n",
    "        steps_val = generator_val.total_samples//self.batchsize\n",
    "        # start the training of the model\n",
    "        self.model.fit(\n",
    "            x=dataset, \n",
    "            batch_size=None,\n",
    "            steps_per_epoch=steps_train, \n",
    "            epochs=self.max_epochs,\n",
    "            verbose=1,\n",
    "            validation_data=dataset_val,\n",
    "            validation_steps=steps_val, \n",
    "            callbacks=[checkpointer, reduce_lr, csv_logger, early_stopping],\n",
    "            max_queue_size=50,\n",
    "            workers=4,\n",
    "            use_multiprocessing=True)\n",
    "        # clear out garbage\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681c0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstantLayerNormalization(Layer):\n",
    "    '''\n",
    "    Class implementing instant layer normalization. It can also be called \n",
    "    channel-wise layer normalization and was proposed by \n",
    "    Luo & Mesgarani (https://arxiv.org/abs/1809.07454v2) \n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        '''\n",
    "            Constructor\n",
    "        '''\n",
    "        super(InstantLayerNormalization, self).__init__(**kwargs)\n",
    "        self.epsilon = 1e-7 \n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "        Method to build the weights.\n",
    "        '''\n",
    "        shape = input_shape[-1:]\n",
    "        # initialize gamma\n",
    "        self.gamma = self.add_weight(shape=shape,\n",
    "                             initializer='ones',\n",
    "                             trainable=True,\n",
    "                             name='gamma')\n",
    "        # initialize beta\n",
    "        self.beta = self.add_weight(shape=shape,\n",
    "                             initializer='zeros',\n",
    "                             trainable=True,\n",
    "                             name='beta')\n",
    " \n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        Method to call the Layer. All processing is done here.\n",
    "        '''\n",
    "\n",
    "        # calculate mean of each frame\n",
    "        mean = tf.math.reduce_mean(inputs, axis=[-1], keepdims=True)\n",
    "        # calculate variance of each frame\n",
    "        variance = tf.math.reduce_mean(tf.math.square(inputs - mean), \n",
    "                                       axis=[-1], keepdims=True)\n",
    "        # calculate standard deviation\n",
    "        std = tf.math.sqrt(variance + self.epsilon)\n",
    "        # normalize each frame independently \n",
    "        outputs = (inputs - mean) / std\n",
    "        # scale with gamma\n",
    "        outputs = outputs * self.gamma\n",
    "        # add the bias beta\n",
    "        outputs = outputs + self.beta\n",
    "        # return output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d21a2",
   "metadata": {},
   "source": [
    "## Traing Start\n",
    "- If you want to train your own model you need to download all files given from: [dataset link](https://github.com/microsoft/DNS-Challenge/tree/master/datasets \"Dataset Link\") and need to put in the **[data]** folder.\n",
    "- If you just want to test the model,Then Just go to the **Testing section** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8587bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the GPU with idx 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "# activate this for some reproducibility\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abad15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to folder containing the noisy or mixed audio training files\n",
    "path_to_train_mix = 'data/train/noisy'\n",
    "# path to folder containing the clean/speech files for training\n",
    "path_to_train_speech = 'data/train/clean'\n",
    "# path to folder containing the noisy or mixed audio validation data\n",
    "path_to_val_mix = 'data/test/noisy'\n",
    "# path to folder containing the clean audio validation data\n",
    "path_to_val_speech = 'data/test/clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ada637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 [(None, None, 257),  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, None, 128)    197632      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 128)    0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 128)    131584      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 257)    33153       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 257)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, None, 257)    0           lambda[0][0]                     \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 512)    0           multiply[0][0]                   \n",
      "                                                                 lambda[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 256)    131072      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "instant_layer_normalization (In (None, None, 256)    512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, None, 128)    197120      instant_layer_normalization[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 128)    0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, None, 128)    131584      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 256)    33024       lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 256)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 256)    0           conv1d[0][0]                     \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 512)    131072      multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None)         0           conv1d_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 986,753\n",
      "Trainable params: 986,753\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# name your training run\n",
    "runName = 'DTLN_model'\n",
    "# create instance of the DTLN model class\n",
    "modelTrainer = DTLN_model()\n",
    "# build the model\n",
    "modelTrainer.build_DTLN_model()\n",
    "# compile it with optimizer and cost function for training\n",
    "modelTrainer.compile_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6cb91",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Note:** if you train with empty folder, you will get errro ! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "modelTrainer.train_model(runName, path_to_train_mix, path_to_train_speech, \\\n",
    "                         path_to_val_mix, path_to_val_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c012bc",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "- first of all you need to run Model classes chell and audio generation classes cell and then you can run test section.\n",
    "- there is two pre train model:\n",
    " - DTLN_norm_500h.h5\n",
    " - DTLN_norm_40h.h5\n",
    "- Both the model present in **saved_model** folder, you just need to load it as instruction provided by below.\n",
    "- In the **data_test** input folder, you need to just past your test .**wav** file and it run the code its saved output file to the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5324cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7bb59b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(model, audio_file_name, out_file_name):\n",
    "    '''\n",
    "    Funtion to read an audio file, rocess it by the network and write the \n",
    "    enhanced audio to .wav file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Keras model\n",
    "        Keras model, which accepts audio in the size (1,timesteps).\n",
    "    audio_file_name : STRING\n",
    "        Name and path of the input audio file.\n",
    "    out_file_name : STRING\n",
    "        Name and path of the target file.\n",
    "\n",
    "    '''\n",
    "    # read audio file with librosa to handle resampling and enforce mono\n",
    "    in_data,fs = librosa.core.load(audio_file_name, sr=16000, mono=True)\n",
    "    # predict audio with the model\n",
    "    predicted = model.predict_on_batch(\n",
    "        np.expand_dims(in_data,axis=0).astype(np.float32))\n",
    "    # squeeze the batch dimension away\n",
    "    predicted_speech = np.squeeze(predicted)\n",
    "    # write the file to target destination\n",
    "    sf.write(out_file_name, predicted_speech,fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba74e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(model, folder_name, new_folder_name):\n",
    "    '''\n",
    "    Function to find .wav files in the folder and subfolders of \"folder_name\",\n",
    "    process each .wav file with an algorithm and write it back to disk in the \n",
    "    folder \"new_folder_name\". The structure of the original directory is \n",
    "    preserved. The processed files will be saved with the same name as the \n",
    "    original file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Keras model\n",
    "        Keras model, which accepts audio in the size (1,timesteps).\n",
    "    folder_name : STRING\n",
    "        Input folder with .wav files.\n",
    "    new_folder_name : STRING\n",
    "        Traget folder for the processed files.\n",
    "\n",
    "    '''\n",
    "    # empty list for file and folder names\n",
    "    file_names = [];\n",
    "    directories = [];\n",
    "    new_directories = [];\n",
    "    # walk through the directory\n",
    "    for root, dirs, files in os.walk(folder_name):\n",
    "        for file in files:\n",
    "            # look for .wav files\n",
    "            if file.endswith(\".wav\"):\n",
    "                # write paths and filenames to lists\n",
    "                file_names.append(file)\n",
    "                directories.append(root)\n",
    "                # create new directory names\n",
    "                new_directories.append(root.replace(folder_name, new_folder_name))\n",
    "                # check if the new directory already exists, if not create it\n",
    "                if not os.path.exists(root.replace(folder_name, new_folder_name)):\n",
    "                    os.makedirs(root.replace(folder_name, new_folder_name))\n",
    "    # iterate over all .wav files             \n",
    "    for idx in range(len(file_names)):\n",
    "        # process each file with the model\n",
    "        process_file(model, os.path.join(directories[idx],file_names[idx]), \n",
    "                     os.path.join(new_directories[idx],file_names[idx]))\n",
    "        print(file_names[idx] + ' processed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a638d5",
   "metadata": {},
   "source": [
    "#### Give all the Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6039d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_stft = True\n",
    "model = 'pretrained_model/DTLN_norm_500h.h5' # this is a model name you can also change it with other saved model\n",
    "in_folder = 'data_test/input' # input folder location\n",
    "out_folder = 'data_test/output' # output folder location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3effbacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if model.find('_norm_') != -1:\n",
    "    norm_stft = True\n",
    "    print(norm_stft)\n",
    "else:\n",
    "    norm_stft = False\n",
    "    print(norm_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "761f4c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               [(None, None, 257),  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, None, 257)]  0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Log (TensorFlowOpLa [(None, None, 257)]  0           tf_op_layer_AddV2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instant_layer_normalization_1 ( (None, None, 257)    514         tf_op_layer_Log[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, None, 128)    197632      instant_layer_normalization_1[0][\n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 128)    0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, None, 128)    131584      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 257)    33153       lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 257)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, None, 257)    0           lambda_3[0][0]                   \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 512)    0           multiply_2[0][0]                 \n",
      "                                                                 lambda_3[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 256)    131072      lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "instant_layer_normalization_2 ( (None, None, 256)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, None, 128)    197120      instant_layer_normalization_2[0][\n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 128)    0           lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, None, 128)    131584      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 256)    33024       lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 256)    0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, None, 256)    0           conv1d_2[0][0]                   \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 512)    131072      multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None)         0           conv1d_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 987,267\n",
      "Trainable params: 987,267\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create class instance\n",
    "modelClass = DTLN_model();\n",
    "# build the model in default configuration\n",
    "modelClass.build_DTLN_model(norm_stft=norm_stft)\n",
    "# load weights of the .h5 file\n",
    "modelClass.model.load_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b3c0af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noisy_input.wav processed successfully!\n",
      "Recording_input.wav processed successfully!\n",
      "record_nilesh_in.wav processed successfully!\n",
      "test_audio.wav processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# process the folder\n",
    "process_folder(modelClass.model, in_folder, out_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
